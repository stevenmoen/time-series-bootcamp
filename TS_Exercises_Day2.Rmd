---
title: "Time Series Exercises"
author: "Steven Moen"
output: html_notebook
---

This file will contain exercises which we will use in class.

## Exercise 1: Finding the Expected Value of a Random Walk with Drift

Compute the expectation of a random walk with drift model given below:

$$
  \begin{aligned}
  x_t = \delta t + \sum_{j=1}^t w_j,
  t = 1,2,....
  \end{aligned}
$$

where $w_t$ is a white noise series defined by:

$$
  \begin{aligned}
  w_t \sim \text{wn}(0, \sigma_m^2)
  \end{aligned}
$$

## Exercise 1 Solution: Finding the Expected Value of a Random Walk with Drift

We begin by taking the expectation of both sides of the equation below

$$
  \begin{aligned}
  \mathbb{E}(x_t) = \mathbb{E}\left(\delta t + \sum_{j=1}^t w_j\right)
  \end{aligned}
$$

An important property of expected values is that it is a linear operator, which is to say that the following is in true in general where $a$ and $b$ are real-valued constants and $X_t$ and $Y_t$ are random variables with finite expected values, support over the entire real line, and indexed by time:

$$
\mathbb{E}(aX_t + bY_t ) = a\mathbb{E}(X_t ) + b\mathbb{E}(Y_t )
$$

Below is a brief proof working directly from the definition of an expectation:

$$
\mathbb{E}(aX_t  + bY_t ) = \int_{-\infty}^{\infty} (aX_t  + bY_t) dt
$$
$$
= a\int_{-\infty}^{\infty} (X_t) dt  + b\int_{-\infty}^{\infty} (Y_t) dt
$$
$$
= \mathbb{E}(X_t ) + b\mathbb{E}(Y_t )
$$

Which completes the proof.

Thus, applying the property proved above, we can say the following:

$$
  \mathbb{E}(x_t) = \mathbb{E}\left(\delta t + \sum_{j=1}^t w_j\right) = \mathbb{E}(\delta t) + \mathbb{E} \left(\sum_{j=1}^t w_j\right)
$$

Noting that in the random walk model, both $\delta$ and $t$ are real valued constants, we know $ \mathbb{E}(\delta t)$. Also, using the linear operator property used above, we can break apart the sum of the noise terms as follows:

$$
\mathbb{E} \left(\sum_{j=1}^t w_j\right) = \mathbb{E}(w_1) + \mathbb{E}(w_2) + ... + \mathbb{E}(w_t)
$$

Note that in our formulation of white noise, we said that $w_t \sim \text{wn}(0, \sigma_m^2)$, thus, the expected value of $w_t$ is 0. 

Synthesizing all of the above facts, we arrive at the solution:

$$
  \mathbb{E}(x_t) = \delta t
$$


## Exercise 2: Finding the Autocovariance of a Random Walk with Drift

Compute the expectation of a random walk with drift model given below:

$$
  \begin{aligned}
  x_t = \delta t + \sum_{j=1}^t w_j,
  t = 1,2,....
  \end{aligned}
$$


where $w_t$ is a white noise series defined by:

$$
  \begin{aligned}
  w_t \sim \text{wn}(0, \sigma_m^2)
  \end{aligned}
$$

## Exercise 2 Solution: Finding the Autocovariance of a Random Walk with Drift

Observe that for a random walk with drift $x_t$, we know that the following is true:

$$
x_t = \sum_{j=1}^t w_j + t \delta
$$

Thus, comparing the series at two different points $s$ and $t$, we have:

$$
\gamma_x(s,t) = cov(x_s, x_t) = cov(\sum_{j=1}^t w_j + t \delta, \sum_{i=1}^s w_i + s \delta)
$$

We know that since $t \delta$ and  $s \delta$ are both constants, the above is equivalent to:

$$
= cov(\sum_{j=1}^t w_j, \sum_{i=1}^s w_i)
$$

Note that for $i \ne j$, we know that the definition of white noise tells us that $cov(w_i, w_j) = 0$. Thus, it follows that the above expression is equal to:

$$
= cov(\sum_{j=1}^t w_j, \sum_{i=1}^s w_i) = \min(s,t) \sigma_w^2
$$

## Exercises 3 and 4: Is a random walk with drift weakly stationary

Are the following time series weakly stationary?

1. $x_t = \delta t + \sum_{j=1}^t w_j, t = 1,2,....$
2. $v_t =  \tfrac{1}{3} (w_{t-1} + w_t + w_{t+1})$

where $w_t$ is a white noise series defined by:

$$
  \begin{aligned}
  w_t \sim \text{wn}(0, \sigma_m^2)
  \end{aligned}
$$

## Exercise 3 Solution

We found earlier that the expected value of a random walk with drift was given by the following:

$$
\mathbb{E}(x_t) = \delta t
$$

Also, the autocovariance is given by:

$$
\gamma_x(s,t) = \min(s,t) \sigma_w^2
$$

For a series to be weakly stationary, the mean function must be constant. We can see from the above mean function that the mean function is non-constant unless $\delta = 0$. Furthermore, the autocovariance function does not depend on $s$ and $t$ only through their differences. Either of these is sufficient to show that a random walk with a drift is not stationary.

## Exercise 4 Solution

Let's start by finding the expected value of the series:

$$
\mathbb{E}(v_t) = \mathbb{E} (\tfrac{1}{3} (w_{t-1} + w_t + w_{t+1}))
$$
$$
= \tfrac{1}{3}\mathbb{E} (w_{t-1} + w_t + w_{t+1}) = \tfrac{1}{3}(\mathbb{E} (w_{t-1}) + \mathbb{E} (w_{t}) + \mathbb{E} (w_{t+1})) = 0
$$

The mean function is constant at 0. Next, let's find the autocovariance function:

$$
\gamma_v(t,t) = \tfrac{1}{9} cov((w_{t-1} + w_t + w_{t+1}), (w_{t-1} + w_t + w_{t+1}))
$$

Recall that for $s \neq t$, $cov(w_s, w_t) = 0$. Thus, the above simplifies to:

$$
\gamma_v(t,t) = \tfrac{1}{9} (cov(w_{t-1}, w_{t-1}) + cov(w_{t}, w_{t}) + cov(w_{t+1}, w_{t+1})) = \tfrac{3}{9}\sigma_w^2
$$

Following similar logic, we know:

$$
\gamma_v(t+1,t) = \tfrac{1}{9} cov((w_{t} + w_{t+1} + w_{t+2}), (w_{t-1} + w_t + w_{t+1}))
$$
$$
= \tfrac{1}{9}(cov(w_{t}, w_{t}) + cov(w_{t+1}, w_{t+1})) = \tfrac{2}{9}\sigma_w^2
$$

This logic can be repeated to show the following solution:

$$
\gamma_v(s,t) = \tfrac{3}{9} \sigma_w^2 \text{ if } s = t
$$
$$
= \tfrac{2}{9} \sigma_w^2 \text{ if } |s-t| = 1
$$
$$
= \tfrac{1}{9} \sigma_w^2 \text{ if } |s-t| = 2
$$
$$
= 0 \text{ if } |s-t| > 2
$$

The above autocovariance analysis shows that the second condition for a weakly stationary time series is met. Thus, this moving average is weakly stationary.


# Question 2.1

Question 2.1a:

Let's fit a regression model on the log returns of Johnson and Johnson stock.

```{r}
# Clear the variables in the enviornment
rm(list = ls())

# Import the package to read the johnson and johnson data.
library(astsa)

# Center the time at 1970
trend = time(jj) - 1970

# Make (Q)uarter factors
Q = factor(cycle(jj) )

# Regression model with no intercept
l_jj_m = lm(log(jj)~0 + trend + Q, na.action=NULL)
summary(l_jj_m)

```

Question 2.1b:

Assuming the model is correct, there is an increase of 0.167 logged earnings per share per year.

Question 2.1c:

```{r}
# Let's figure out the difference between the Q3 and Q4 coefficients
l_jj_m$coefficients

# Let's take the difference between the coefficients
l_jj_m$coefficients[5] - l_jj_m$coefficients[4]
# Decreases by 0.269

# Let's take the percentage change
(l_jj_m$coefficients[5]/l_jj_m$coefficients[4]) - 1
# Decreases by 23.3%
```

The average log earnings rate decreases by 0.269, or about 23.3%.

Question 2.1d:

Let's add an intercept term to the model in 2.1a.

```{r}
# Regression model with an added intercept
l_jj_m_int = lm(log(jj)~ trend + Q, na.action=NULL)
summary(l_jj_m_int)
```

There is a problem because the intercept took on the value for Q1, except unlike in the original model, the intercept term is added in all quarterly observations. Therefore, this obscures the meanings (and the statistical significance) of the Q2, Q3, and Q4 covariates.

Question 2.1e:

Let's graph the logged stock price returns and the fitted values along with their residuals.

```{r}
# Let's plot the log EPS of JJ and the fitte values
plot(log(jj), ylab = "Log EPS of JJ", 
     main = "Log EPS of JJ (Solid) and Modeled Values (Dashed)")
lines(fitted(l_jj_m), lty = 2)

# Let's plot the residuals
plot(l_jj_m$residuals, ylab = "Model Residuals", main = "Model Residuals Over Time")
abline(h=0, lty = 2)

```

While the model appears to fit well, it does not appear to me that the residuals are white. There is a period from the early 1960's to about 1965 where the residuals
drop below zero, whereas in 1970 the residuals go well above 0 and stay there there until about 1975. Also, variance appears higher in the late 1960s and the early 1960s.

# Question 2.3

Question 2.3a:

Let's plot four random walks with drift.

```{r}
# Let's set the seed for reproducibility
set.seed(21)

# Let's set up for the graphs
par(mfrow=c(2,2), mar=c(2.5,2.5,0,0)+.5, mgp=c(1.6,.6,0))

# Loop through 4 random walks with drift
for (i in 1:4){
  # Create the x series
  x = ts(cumsum(rnorm(100,.01,1)))
  # Run the regression
  regx = lm(x~0+time(x), na.action=NULL) 
  # Plot the data
  plot(x, ylab='Random Walk w/ Drift') 
  # Plot the drift
  abline(a=0, b=.01, col=2, lty=2) 
  # Plot the regression
  abline(regx, col=4)
}
```

Question 2.3b:

Let's plot four linear trends with a noise term added.

```{r}
# Let's set the seed for reproducibility
set.seed(21)

# Let's set up for the graphs
par(mfrow=c(2,2), mar=c(2.5,2.5,0,0)+.5, mgp=c(1.6,.6,0))

# Loop through 4 linear trends with noise added
for (i in 1:4){
  # Create the x series
  x_lt = ts(cumsum(0.01 + rnorm(100,0,1)))
  # Run the regression
  regx_lt = lm(x_lt~0+time(x_lt), na.action=NULL) 
  # Plot the data
  plot(x_lt, ylab='Linear Trend + Noise Term') 
  # Plot the drift
  abline(a=0, b=.01, col=2, lty=2) 
  # Plot the regression
  abline(regx_lt, col=4)
}
```

Question 2.3c:

It appears that these two series are equivalent when using the same seed to produce the same rnorm pulls. This shows how there can be massive deviations from the red trend line even with standard normal white noise.

# Question 2.8

Question 2.8a:

Let's plot the glacial varve data.

```{r}
# Let's examine the sample variance for the first half 
# and the second half of the data

# 1st half variance
var_1h <- var(varve[1:(length(varve)/2)])

# 2nd half variance
var_2h <- var(varve[((length(varve)/2)+1):length(varve)])

# Print the two variances
print(var_1h)
print(var_2h)

# The variances are clearly unequal, but let's check if they are significantly different
var.test(varve[1:(length(varve)/2)], 
         varve[((length(varve)/2)+1):length(varve)])
# According to the f-test, the variances are significantly different at the 95% level

# Now let's do the same for variance of the log

# 1st half variance of log data
var_1h_l <- var(log(varve[1:(length(varve)/2)]))

# 2nd half variance of log data
var_2h_l <- var(log(varve[((length(varve)/2)+1):length(varve)]))

# Print the two variances of the logged data
print(var_1h_l)
print(var_2h_l)

# Let's test if the differences are significantly different
var.test(log(varve[1:(length(varve)/2)]), 
         log(varve[((length(varve)/2)+1):length(varve)]))
# The variances are significantly different at the 95% level, 
# but the variances are closer

# Let's plot the histograms of the glacial varve data
par(mfrow=c(1,2))
hist(varve, main="varve", ylab="")
hist(log(varve), main="log(varve)", ylab="" )

# The variance is more stable here

```

While the variances between the 1st and 2nd halves of the data are still unequal, taking the log of the data makes the data more closely resemble a normal distribution.

Question 2.8b:

Let's plot the time series and compare it to global temperature records.

```{r}
# Let's plot the glacial varve data
par(mfrow=c(1,1))
plot(log(varve), main="log(varve)", ylab="")

# Let's also plot the global temperature data
plot(globtemp, type="o", ylab="Global Temperature Deviations")
```

It looks like there is a material increase in temperature occuring between about 300 and about 425 or 450, which is to say that there is evidence of a temperature increase similar to that seen between 1945 and 2015 in the second plot (which is the same as figure 1.2 in the textbook).

Question 2.8c:

Let's look at the sample ACF of log(varve).

```{r}
acf(log(varve))
```

There is significant autocovariance in these logged data, and the lags do not decline materially in significance even going out past lag 25, which shows that one year tends to be strongly correlated with prior years.

Question 2.8d:

Let's plot the logged difference and the ACF of the logged difference.

```{r}
par(mfrow=c(2,1))
# Time plot
plot(diff(log(varve)), main="diff(log(varve)", ylab="")

# ACF
acf(diff(log(varve)))
```

The data looks stationary now. Only the first lag of the ACF looks significant and the time series plot looks stable.
A practical interpretation of $\mu_t$ is that it's the percentage change of the temperature.

Question 2.8e and 2.8f are answered on seperate paper.

# Question 2.9

Question 2.9a:

Let's look at a regression on $S_t$ on time $t$. 

```{r}
# Let's fit the model between SOI and time
soi_t_m <- lm(soi ~ time(soi))
summary(soi_t_m)

# The coefficient on time is statistically significant from 0,
# but the effect looks small

# Let's plot this line against the SOI
par(mfrow = c(1,1))
plot(soi, ylab="", xlab="", main="Southern Oscillation Index") 
abline(soi_t_m, col=4)
```

While the effect of time on the SOI is significantly different from 0 at the 1% confidence level, it's not clear that there's a huge effect of the time coefficient on the SOI. Not only does the slope appear fairly close to 0, there is a lot of noise not captured by the regression model.

Question 2.9b:

Let's plot the periodogram.

```{r}
# Let's combine time with the residuals
soi_resid = ts.intersect(time(soi), residuals(soi_t_m))

# Let's plot the residuals on the periodigram
#soi_spec2 = mvspec(residuals(soi_t_m), log="yes")
soi_spec = mvspec(soi_resid, log="no", col = 1, lty = 1)
```

It looks like there's secondary peaks at 2 and 3, with 3 being slighly more pronounced. My hunch is that the 3 indicates the el niño cycle.

# Question 2.10

Question 2.10a:

Let's plot the time series "oil" and "gas".

```{r}
plot.ts(gas, ylim = c(0,400), ylab = "Oil ($/bbl) and Gas Prices (cents/gal)",
        main = "Oil (Dashed) and Gas (Solid) Prices")
lines(oil, lty = 2)
```

To me, these time series look like random walks with a drift seen in Section 1. They do not appear stationary; indeed, the mean looks non-constant (the mean generally looks to increase over time) and the autovovariance changes in periods of economic stress, such as in 2008-2010.

Question 2.10b:

Transforming the data by differencing the log prices would likely simulate a percentage change well, because there is (usually) not a large weekly change which would make the higher order terms in the Taylor series expansion significant. Doing such a transformation would allow the statistician to make more accurate conclusions about these data because the would likely be closer to stationary.

Question 2.10c:

Let's transform the data by taking the difference of the logarithm of the prices.

```{r}
plot.ts(diff(log(gas)), ylab = "Diff. Log Oil and Gas Prices",
        main = "Diff. Log Oil (Red) and Diff. Log Gas (Blue) Prices", col = 4)
lines(diff(log(oil)), col = 2)

# These data look much more stationary

# Let's look at the ACFs of these transformed data

acf(diff(log(gas)))
acf(diff(log(oil)))

# Only a few slightly significant lags
```

After transforming these data, they look much closer to a stationary time series than they did before. Indeed, the mean seems to stay constant and the autocorrelation for most lags are not statistically significant at the 5% level (or marginally significant).

Question 2.10d:

Let's look at the CCF between the differenced log values of oil and gas.

```{r}
# ccf(diff(log(gas)), diff(log(oil)), ylab='CCovF', type='covariance')
# LHS is gas leading, RHS is oil leading
ccf(diff(log(gas)), diff(log(oil)), 48, main="Diff. Log Gas vs Diff. Log Oil", ylab="CCF")
```

There is some evidence that the lags of the transformed gas prices are correlated with future transformed oil prices. Having said that, just because the lags may be different from 0 at the 95% confidence level doeesn't mean that they are economically significant.

Question 2.10e:

Let's do scatterplots of gas vs. oil prices with lags of gas prices.

```{r}
# Gas lagged one week - one week of lead time with oil
plot(diff(log(oil)), lag(diff(log(gas)),1), 
     xlab = "Diff. Log. Oil Prices (1-week lead)",
     ylab = "Diff. Lag Gas Prices", 
     main = "Diff. Log. Gas vs. Leading Diff. Log Oil Prices with Lowess Smoother")
lines(lowess(diff(log(oil)), lag(diff(log(gas)),1)))

# There is generally a linear fit in these data but the association isn't 
# very strong and there are some outliers and leverage points to contend
# with 

# Gas lagged two weeks - two weeks of lead time with oil
plot(diff(log(oil)), lag(diff(log(gas)),2), 
     xlab = "Diff. Log. Oil Prices (2-week lead)",
     ylab = "Diff. Lag Gas Prices", 
     main = "Diff. Log. Gas vs. Leading Diff. Log Oil Prices with Lowess Smoother")
lines(lowess(diff(log(oil)), lag(diff(log(gas)),2)))

# Similar story as with 1-week lead time

# Gas lagged three weeks - three weeks of lead time with oil
plot(diff(log(oil)), lag(diff(log(gas)),3), 
     xlab = "Diff. Log. Oil Prices (3-week lead)",
     ylab = "Diff. Lag Gas Prices", 
     main = "Diff. Log. Gas vs. Leading Diff. Log Oil Prices with Lowess Smoother")
lines(lowess(diff(log(oil)), lag(diff(log(gas)),3)))

# Similar story as with 1-week and 2-week lead times
```

Generally speaking, there is a linear fit between the leading transformed oil prices and the transformed gas prices, though in all of the plots there is one noticible outlier (with a high transformed gas price but typical transformed oil price) and one noticable leverage point (with a high transformed oil price but a typical transformed gas price).

Question 2.10f, i:

Let's look at the regression to see if there's asymmetry between oil and gas prices.

```{r}
# Transformed price of oil
poil = diff(log(oil))

# Transformed price of gas
pgas = diff(log(gas))

# Indicator; 0 if oil price drops, 1 if it increases
indi = ifelse(poil < 0, 0, 1)

# Building a dataset
mess = ts.intersect(pgas, poil, poilL = lag(poil,-1), indi)
# head(mess)

# Fitting the regression
summary(fit <- lm(pgas~ poil + poilL + indi, data=mess))
```

The model seems to fit well from looking at the summary. The variables are all seperately significantly different from 0 at the 5% level, and the R-squared shows that the model explains roughly 45% of variation in the transformed gas price.

Question 2.10f, ii:

When the price of oil is dropping at time t, the fitted model is pgas ~ poil + poilL. When there is no growth or positive growth, the model is pgas ~ poil + poilL + indi. It appears that there is a higher response to oil prices when they're rising compared to when they're falling.

Question 2.10f, iii:

Let's look at the residuals.

```{r}
plot(fit$residuals)
abline(h=0)
```

The residuals generally look like white noise, with a pocket of heteroskedasticity around 450 - 470. I don't see any pronounced issues.

# Question 2.11

Let's smooth the global temperature data with two different methods.

```{r}
# Let's start with the MA smoother

# Create the weights
wgts = c(.5, rep(1,11), .5)/12

# Add the weights to a filter
globtempf = filter(globtemp, sides=2, filter=wgts)
plot(globtemp, main = "Global Temperature Data with MA Smoother")
lines(globtempf, lwd=2, col=4)
# par(fig = c(.65, 1, .65, 1), new = TRUE)
nwgts = c(rep(0,20), wgts, rep(0,20))
# plot(nwgts, type="l", ylim = c(-.02,.1), xaxt='n', yaxt='n', ann=FALSE)

# Let's now look at a different type of smoother, the lowess method
par(mfrow=c(1,1))
plot(globtemp, main = "Global Temperature Data with Lowess Smoother")

# Let's use the lowess with the default span
lines(lowess(globtemp), lty=2, lwd=2, col=2) # trend (with default span)
```

The lowess smoother definitely looks smoother, but misses some of the features of the data that the moving average filter captures. I think that the lowess smoother is a better choice to illustrate an overall trend to an audience because the eye can follow the trend more easily than with the MA filter.
Having said that, an analysis digging into the more in-depth movements may appreciate the MA filter.